{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain_core\n",
      "  Downloading langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.41.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.32-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.10.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.100-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.9/site-packages (from langchain_core) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.9/site-packages (from langchain_core) (4.12.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.9/site-packages (from openai) (4.66.5)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.7-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.20.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.7.24)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.33-py3-none-any.whl (391 kB)\n",
      "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.41.1-py3-none-any.whl (362 kB)\n",
      "Downloading langchain_openai-0.1.22-py3-none-any.whl (51 kB)\n",
      "Downloading aiohttp-3.10.5-cp39-cp39-macosx_11_0_arm64.whl (389 kB)\n",
      "Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading jiter-0.5.0-cp39-cp39-macosx_11_0_arm64.whl (283 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.100-py3-none-any.whl (148 kB)\n",
      "Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Downloading pydantic_core-2.20.1-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading SQLAlchemy-2.0.32-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.7.0-cp39-cp39-macosx_11_0_arm64.whl (907 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m907.9/907.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading orjson-3.10.7-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (251 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, SQLAlchemy, sniffio, pydantic-core, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, jiter, h11, frozenlist, distro, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, typing-inspect, tiktoken, pydantic, jsonpatch, httpcore, anyio, aiosignal, httpx, dataclasses-json, aiohttp, openai, langsmith, langchain_core, langchain-text-splitters, langchain-openai, langchain, langchain_community\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "Successfully installed SQLAlchemy-2.0.32 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.4.0 async-timeout-4.0.3 attrs-24.2.0 dataclasses-json-0.6.7 distro-1.9.0 frozenlist-1.4.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-openai-0.1.22 langchain-text-splitters-0.2.2 langchain_community-0.2.12 langchain_core-0.2.33 langsmith-0.1.100 marshmallow-3.22.0 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 openai-1.41.1 orjson-3.10.7 pydantic-2.8.2 pydantic-core-2.20.1 sniffio-1.3.1 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_core langchain_community openai langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/data-science/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RefineDocumentsChain, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from openai import AzureOpenAI\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining Prompts\n",
    "# document_prompt = PromptTemplate(\n",
    "#     input_variables=[\"page_content\"],\n",
    "#      template=\"{page_content}\"\n",
    "# )\n",
    "document_variable_name = \"context\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this content: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_refine = PromptTemplate.from_template(\n",
    "    \"Here's your first summary: {prev_response}. \"\n",
    "    \"Now add to it based on the following context: {context}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating Dummy Data\n",
    "dummy = pd.DataFrame({\"keys\":[\" Gather all ingredients. Preheat the oven to 350 degrees F (175 degrees C). Grease and flour a 9-inch square cake pan. \",\" Cream sugar and butter together in a mixing bowl. Add eggs, one at a time, beating briefly after each addition. Mix in vanilla. \", \" Combine flour and baking powder in a separate bowl. Add to the wet ingredients and mix well. Add milk and stir until smooth. \"], \"values\": [1,2,3]})\n",
    "dummy_docs = DataFrameLoader(dummy,page_content_column=\"keys\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading custom LLM wrapper chain class\n",
    "import llm\n",
    "from imp import reload\n",
    "reload(llm)\n",
    "summary = llm.chain(initial_prompt=prompt, refine_prompt=prompt_refine, documents=dummy_docs, time_delay_seconds=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content continues with the baking process, instructing to combine flour and baking powder in a separate bowl. This dry mixture is then added to the previously prepared wet ingredients and mixed thoroughly. The recipe then calls for the addition of milk, which should be stirred into the mixture until it achieves a smooth consistency.\n"
     ]
    }
   ],
   "source": [
    "## summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Given a detailed service event report about an engine or vehicle failure, extract the following information from the text and format the result as a python dictionary with JSON formatting standards. This means surround all properties in double quotes instead of single:\n",
    "\n",
    "        With the key of \"model_identified_complaint\", extract the primary customer complaint. The complaint is defined as the main engine or vehicle issue that the customer or technician is claiming is causing the repair, and summarize the complaint in less than 4 words, including the exact part or system affected, using technical terms.\n",
    "If no such information is found, mention \"Not available\". in the response, only keep the relevant information. Avoid narriative explainations. Only include engine related terms and complaints. Do not count P codes as a complaint. Prioritize the first mentioned complaint, and if there is something mentioned by the customer prioritize that. If the text contains the phrase \"maint\" or similar to \"maintenance\", output the complaint as maintenance. For the outputted complaint, fix any spelling mistakes. If the identified complaint is in another language, translate it to English. Remove any quotations and extra punctuation, just keep the alphanumeric characters. Output the result in lowercase.\n",
    "With the key of \"identified_cause\", extract the assignable cause of repair if any are mentioned in the text. For example: Belt noise, no communication, no start, sensor malfunctioning. This cause should identify the component or the cause that was fixed that able to fix the issue during the repair.\n",
    "only keep the relevant information and avoid narrative explanations.\n",
    "With the key of \"identified_repairs\", extract and present the repair actions in a short summary, For example: replaced nox sensor, replaced egr cooler, fixed coolant leak, in the response and only keep the relevant information while avoiding narrative explanations.\n",
    "With the key of \"identified_keywords\", extract important service, repair or symptom related keywords in a comma separated list. Focus on any objective nouns that are mentioned. Examples: loose connector, burning smell, P codes or U codes.\n",
    "in the response, only keep the relevant information and avoid any narrative explanations.\n",
    "With the key of \"identified_codes\", given a detailed service event report about an engine or vehicle failure, extract ONLY P codes or U codes that start with a P or U and have 4 digits and truncate values before hyphens if any are present. Codes with hyphens like U0100-00 or P0100-00 should be truncated to U0100, and P0100 respectively. For example text might mention U0100, P2580 etc. If there are no such codes mentioned in Combined Narrative: , return \"Not Found\"\n",
    "Do not include the prefix of \"P Code and U Code:\" in the response, only keep the relevant information and avoid narrative explanations. If none are found, return \"Not found\".\n",
    "output the final combined response in the format of a python dictionary, with the keys of the dictionary being the following: \"complaint\", \"cause\", \"repairs\", \"keywords\", \"codes\", and the values being the answers to the corresponding information above.\n",
    "\n",
    "     {context}\n",
    "     \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given a detailed service event report about an engine or vehicle failure, extract the following information from the text and format the result as a python dictionary with JSON formatting standards. This means surround all properties in double quotes instead of single:\\n\\n        With the key of \"model_identified_complaint\", extract the primary customer complaint. The complaint is defined as the main engine or vehicle issue that the customer or technician is claiming is causing the repair, and summarize the complaint in less than 4 words, including the exact part or system affected, using technical terms.\\nIf no such information is found, mention \"Not available\". in the response, only keep the relevant information. Avoid narriative explainations. Only include engine related terms and complaints. Do not count P codes as a complaint. Prioritize the first mentioned complaint, and if there is something mentioned by the customer prioritize that. If the text contains the phrase \"maint\" or similar to \"maintenance\", output the complaint as maintenance. For the outputted complaint, fix any spelling mistakes. If the identified complaint is in another language, translate it to English. Remove any quotations and extra punctuation, just keep the alphanumeric characters. Output the result in lowercase.\\nWith the key of \"identified_cause\", extract the assignable cause of repair if any are mentioned in the text. For example: Belt noise, no communication, no start, sensor malfunctioning. This cause should identify the component or the cause that was fixed that able to fix the issue during the repair.\\nonly keep the relevant information and avoid narrative explanations.\\nWith the key of \"identified_repairs\", extract and present the repair actions in a short summary, For example: replaced nox sensor, replaced egr cooler, fixed coolant leak, in the response and only keep the relevant information while avoiding narrative explanations.\\nWith the key of \"identified_keywords\", extract important service, repair or symptom related keywords in a comma separated list. Focus on any objective nouns that are mentioned. Examples: loose connector, burning smell, P codes or U codes.\\nin the response, only keep the relevant information and avoid any narrative explanations.\\nWith the key of \"identified_codes\", given a detailed service event report about an engine or vehicle failure, extract ONLY P codes or U codes that start with a P or U and have 4 digits and truncate values before hyphens if any are present. Codes with hyphens like U0100-00 or P0100-00 should be truncated to U0100, and P0100 respectively. For example text might mention U0100, P2580 etc. If there are no such codes mentioned in Combined Narrative: , return \"Not Found\"\\nDo not include the prefix of \"P Code and U Code:\" in the response, only keep the relevant information and avoid narrative explanations. If none are found, return \"Not found\".\\noutput the final combined response in the format of a python dictionary, with the keys of the dictionary being the following: \"complaint\", \"cause\", \"repairs\", \"keywords\", \"codes\", and the values being the answers to the corresponding information above.\\n\\n     testing\\n     '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(context = \"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
